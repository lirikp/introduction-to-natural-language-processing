{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация текста\n",
    "\n",
    "Одной из широко используемых задач NLP в различных бизнес-задачах является “классификация текста”. Цель классификации текста состоит в том, чтобы автоматически классифицировать текстовые документы по одной или нескольким определенным категориям. Некоторые примеры классификации текста:\n",
    "- Определенеие настроений аудитории из социальных сетей\n",
    "- Обнаружение спама и нежелательных писем\n",
    "- Автоматическая маркировка запросов клиентов\n",
    "- Категоризация новостных статей по определенным темам\n",
    "\n",
    "Классификация текста является примером задачи машинного обучения с учителем, поскольку для обучения классификатора используется помеченный набор данных, содержащий текстовые документы и их метки. Пайплайн классификации текста состоит из 4 основных частей:\n",
    "1. Подготовка данных\n",
    "2. Генерация признаков\n",
    "3. Тренировка модели\n",
    "4. Оценка и тьюнинг модели\n",
    "\n",
    "Проделаем данные шаги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Professional\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Professional\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Professional\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import pandas, numpy, textblob, string\n",
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка датасета\n",
    "\n",
    "В качестве датасета возьмем набор данных Amazon reviews. Набор данных состоит из 3,6 млн текстовых обзоров и их меток, мы будем использовать только небольшую часть данных. Датасет содержит 'text' текст отзыва и 'label' метку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "data = open('datasets/corpus_HW6').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text       label\n0  Stuning even for the non-gamer: This sound tra...  __label__2\n1  The best soundtrack ever to anything.: I'm rea...  __label__2\n2  Amazing!: This soundtrack is my favorite music...  __label__2\n3  Excellent Soundtrack: I truly like this soundt...  __label__2\n4  Remember, Pull Your Jaw Off The Floor After He...  __label__2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Stuning even for the non-gamer: This sound tra...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The best soundtrack ever to anything.: I'm rea...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Amazing!: This soundtrack is my favorite music...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Excellent Soundtrack: I truly like this soundt...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n      <td>__label__2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка текста\n",
    "\n",
    "Сделаем некоторую базовую предобработку текста: \n",
    "- уберем апострофы\n",
    "- уберем специальные символы\n",
    "- приведем к нижнему регистру\n",
    "- сделаем токенизацию\n",
    "- уберем стоп-слова \n",
    "- сделаем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeApostrophe(review):\n",
    "    review = re.sub(r\"won't\", \"will not\", review)\n",
    "    review = re.sub(r\"can\\'t\", \"can not\", review)\n",
    "    review = re.sub(r\"n\\'t\", \" not\", review)\n",
    "    review = re.sub(r\"\\'re\", \" are\", review)\n",
    "    review = re.sub(r\"\\'s\", \" is\", review)\n",
    "    review = re.sub(r\"\\'d\", \" would\", review)\n",
    "    review = re.sub(r\"\\'ll\", \" will\", review)\n",
    "    review = re.sub(r\"\\'t\", \" not\", review)\n",
    "    review = re.sub(r\"\\'ve\", \" have\", review)\n",
    "    review = re.sub(r\"\\'m\", \" am\", review)\n",
    "    return review\n",
    "\n",
    "def removeApostropheFixed(review):\n",
    "    review = re.sub(r\"won't\", \"willnot\", review)\n",
    "    review = re.sub(r\"can\\'t\", \"cannot\", review)\n",
    "    review = re.sub(r\"\\snot\", \"not\", review)\n",
    "    review = re.sub(r\"n\\'t\", \"not\", review)\n",
    "    review = re.sub(r\"\\'re\", \" are\", review)\n",
    "    review = re.sub(r\"\\'s\", \" is\", review)\n",
    "    review = re.sub(r\"\\'d\", \" would\", review)\n",
    "    review = re.sub(r\"\\'ll\", \" will\", review)\n",
    "    review = re.sub(r\"\\'t\", \" not\", review)\n",
    "    review = re.sub(r\"\\'ve\", \" have\", review)\n",
    "    review = re.sub(r\"\\'m\", \" am\", review)\n",
    "    return review\n",
    "\n",
    "def removeSpecialChars(review):\n",
    "     return re.sub('[^a-zA-Z]', ' ', review)\n",
    "    \n",
    "def doCleaningFixed(review):\n",
    "    review = removeApostropheFixed(review)\n",
    "    review = removeSpecialChars(review) \n",
    "    review = review.lower()  \n",
    "    review = review.split() #Tokenization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    review = [lmtzr.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = \" \".join(review)    \n",
    "    return review\n",
    "\n",
    "def doTextCleaning(review):\n",
    "    review = removeApostrophe(review)\n",
    "    review = removeSpecialChars(review) \n",
    "    review = review.lower()  \n",
    "    review = review.split() #Tokenization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    review = [lmtzr.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = \" \".join(review)    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I won't fdfd \\'m  ase\\'re can\\'t this won't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'I will not fdfd  am  ase are can not this will not'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeApostrophe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['textOld'] = trainDF['text'].apply(doTextCleaning)\n",
    "trainDF['textNew'] = trainDF['text'].apply(doCleaningFixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0     stuning even non gamer sound track beautiful p...\n1     best soundtrack ever anything reading lot revi...\n2     amazing soundtrack favorite music time hand in...\n3     excellent soundtrack truly like soundtrack enj...\n4     remember pull jaw floor hearing played game kn...\n5     absolute masterpiece quite sure actually takin...\n6     buyer beware self published book want know rea...\n7     glorious story loved whisper wicked saint stor...\n8     five star book finished reading whisper wicked...\n9     whisper wicked saint easy read book made want ...\n10    worst complete waste time typographical error ...\n11    great book great book could put could read fas...\n12    great read thought book brilliant yet realisti...\n13    oh please guess romance novel lover one discer...\n14    awful beyond belief feel write keep others was...\n15    try fool u fake review glaringly obvious glowi...\n16    romantic zen baseball comedy hear folk say mak...\n17    fashionable compression stocking dvt doctor re...\n18    jobst ultrasheer thigh high excellent product ...\n19    size recomended size chart real size much smal...\nName: textOld, dtype: object"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['textOld'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0     stuning even non gamer sound track beautiful p...\n1     best soundtrack ever anything reading lot revi...\n2     amazing soundtrack favorite music time hand in...\n3     excellent soundtrack truly like soundtrack enj...\n4     remember pull jaw floor hearing played game kn...\n5     absolute masterpiece quite sure actually takin...\n6     buyer beware self published book want know rea...\n7     glorious story loved whisper wicked saint stor...\n8     five star book finished reading whisper wicked...\n9     whisper wicked saint easy read book made want ...\n10    worst complete waste time typographical error ...\n11    great book great book couldnot put couldnot re...\n12    great read thought book brilliant yet realisti...\n13    oh please guess romance novel lover one andnot...\n14    awful beyond belief feel write keep others was...\n15    donot try fool u fake review glaringly obvious...\n16    romantic zen baseball comedy hear folk say don...\n17    fashionable compression stocking dvt doctor re...\n18    jobst ultrasheer thigh high excellent product ...\n19    size recomended size chart arenot real size mu...\nName: textNew, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['textNew'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['textOld'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "На этом этапе необработанные текстовые данные будут преобразованы в векторы признаков, а новые признаки будут созданы с использованием существующего набора данных. Мы реализуем следующие идеи, чтобы получить релевантные признаки из нашего набора данных:\n",
    "\n",
    "- Count вектора как признаки\n",
    "- TF-IDF вектора как признаки\n",
    "  - Word level\n",
    "  - N-Gram level\n",
    "  - Character level\n",
    "- Text / NLP признаки\n",
    "\n",
    "##### Count Vectors\n",
    "Count Vector - это матричная нотация набора данных, в которой каждая строка представляет документ из корпуса, каждый столбец представляет термин из корпуса, а каждая ячейка представляет счетчик частот конкретного термина в конкретном документе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['textOld'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF вектора\n",
    "\n",
    "Оценка TF-IDF представляет собой относительную важность термина в документе и во всем корпусе. \n",
    "\n",
    "TF (t) = (количество раз, когда термин t появляется в документе) / (общее количество терминов в документе)\n",
    "IDF (t) = log_e(общее количество документов / количество документов с термином t в нем).\n",
    "\n",
    "Векторы TF-IDF могут быть сгенерированы на разных уровнях входных токенов (слова, символы, n-граммы).\n",
    "\n",
    "- Word Level TF-IDF : матрица содержит tf-idf оценки каждого слова в разных документах \n",
    "- N-gram Level TF-IDF : N-грам - комбинация N слов вместе. Матрица содержит tf-idf оценки N-грамов.\n",
    "- Character Level TF-IDF : матрица содержит tf-idf оценки N-грам символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['textOld'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['textOld'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['textOld'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "matrix([[0.05037352, 0.        , 0.        , ..., 0.        , 0.        ,\n         0.        ],\n        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n         0.        ],\n        [0.02669317, 0.        , 0.        , ..., 0.        , 0.        ,\n         0.        ],\n        ...,\n        [0.02994059, 0.        , 0.        , ..., 0.        , 0.        ,\n         0.        ],\n        [0.12653047, 0.        , 0.        , ..., 0.        , 0.        ,\n         0.        ],\n        [0.01814534, 0.        , 0.        , ..., 0.        , 0.        ,\n         0.        ]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf_ngram_chars.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "label\n__label__1    74.0\n__label__2    68.0\nName: word_count, dtype: float64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.groupby(\"label\")['word_count'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text / NLP признаки\n",
    "\n",
    "Можно создать ряд дополнительных текстовых функций, которые иногда полезны для улучшения моделей классификации текста. Вот некоторые примеры:\n",
    "\n",
    "1. Word Count документов – Общее количество слов в документе\n",
    "2. Character Count документов – общее количество символов в документе\n",
    "3. Average Word Density документов – средняя длина слов в документе\n",
    "4. Puncutation Count – общее количество символов пунктуции в документе\n",
    "5. Upper Case Count – общее количество слов с верхним регистром в документе\n",
    "6. Title Word Count – общее количество заголовков  в документе\n",
    "7. Частотное распределение тэгов частей речи:\n",
    "  - Noun Count - количество существительных\n",
    "  - Verb Count - количество глаголов\n",
    "  - Adjective Count - количество прилагательных\n",
    "  - Adverb Count - количество наречий\n",
    "  - Pronoun Count - количество местоимений\n",
    "  \n",
    "Эти признаки довольно экспериментальны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      char_count  word_count  word_density  punctuation_count  \\\n0            426          80      5.259259                 11   \n1            509          97      5.193878                 14   \n2            760         129      5.846154                 40   \n3            743         118      6.243697                 33   \n4            481          87      5.465909                 22   \n...          ...         ...           ...                ...   \n9995         867         152      5.666667                 25   \n9996         861         141      6.063380                 14   \n9997         650         108      5.963303                 17   \n9998         135          27      4.821429                  6   \n9999         536         103      5.153846                 13   \n\n      title_word_count  upper_case_word_count  \n0                   10                      3  \n1                    7                      3  \n2                   24                      4  \n3                   52                      4  \n4                   30                      0  \n...                ...                    ...  \n9995                14                      3  \n9996                16                      0  \n9997                11                      0  \n9998                 2                      1  \n9999                23                      4  \n\n[10000 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>char_count</th>\n      <th>word_count</th>\n      <th>word_density</th>\n      <th>punctuation_count</th>\n      <th>title_word_count</th>\n      <th>upper_case_word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>426</td>\n      <td>80</td>\n      <td>5.259259</td>\n      <td>11</td>\n      <td>10</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>509</td>\n      <td>97</td>\n      <td>5.193878</td>\n      <td>14</td>\n      <td>7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>760</td>\n      <td>129</td>\n      <td>5.846154</td>\n      <td>40</td>\n      <td>24</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>743</td>\n      <td>118</td>\n      <td>6.243697</td>\n      <td>33</td>\n      <td>52</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>481</td>\n      <td>87</td>\n      <td>5.465909</td>\n      <td>22</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>867</td>\n      <td>152</td>\n      <td>5.666667</td>\n      <td>25</td>\n      <td>14</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>861</td>\n      <td>141</td>\n      <td>6.063380</td>\n      <td>14</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>650</td>\n      <td>108</td>\n      <td>5.963303</td>\n      <td>17</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>135</td>\n      <td>27</td>\n      <td>4.821429</td>\n      <td>6</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>536</td>\n      <td>103</td>\n      <td>5.153846</td>\n      <td>13</td>\n      <td>23</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[['char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'upper_case_word_count' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# подсчет тэгов частей речи в предложении\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      noun_count  verb_count  adj_count  adv_count  pron_count\n0             20          15          6          6          11\n1             20          23          9          3          10\n2             39          18         13         10          11\n3             52          12          9          2           7\n4             31          13          7          2           9\n...          ...         ...        ...        ...         ...\n9995          46          24         13         10           5\n9996          49          15         14          5           3\n9997          30          18         14          2           8\n9998           3          10          1          4           4\n9999          29          21          6          4          13\n\n[10000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>noun_count</th>\n      <th>verb_count</th>\n      <th>adj_count</th>\n      <th>adv_count</th>\n      <th>pron_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20</td>\n      <td>15</td>\n      <td>6</td>\n      <td>6</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>23</td>\n      <td>9</td>\n      <td>3</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>39</td>\n      <td>18</td>\n      <td>13</td>\n      <td>10</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>52</td>\n      <td>12</td>\n      <td>9</td>\n      <td>2</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31</td>\n      <td>13</td>\n      <td>7</td>\n      <td>2</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>46</td>\n      <td>24</td>\n      <td>13</td>\n      <td>10</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>49</td>\n      <td>15</td>\n      <td>14</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>30</td>\n      <td>18</td>\n      <td>14</td>\n      <td>2</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>3</td>\n      <td>10</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>29</td>\n      <td>21</td>\n      <td>6</td>\n      <td>4</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF[['noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение моделей\n",
    "\n",
    "Последний шаг в процессе классификации текста - это обучение классификатора с использованием признаков, созданных на предыдущем шаге. Опробуем несколько вариантов моделей машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes\n",
    "\n",
    "Наивный Байес - это метод классификации, основанный на теореме Байеса с допущением независимости между предикторами. Наивный байесовский классификатор предполагает, что наличие определенного признака в классе не связано с наличием какого-либо другого признака в нем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.8424\n",
      "NB, WordLevel TF-IDF:  0.8428\n",
      "NB, N-Gram Vectors:  0.7816\n",
      "NB, CharLevel Vectors:  0.8052\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes на Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes на Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes на Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes на Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 1, ..., 0, 0, 0])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = naive_bayes.MultinomialNB()\n",
    "classifier.fit(xtrain_tfidf_ngram_chars, train_y)    \n",
    "predictions = classifier.predict(xvalid_tfidf_ngram_chars)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Linear Classifier (Логистическая регрессия)\n",
    " \n",
    "Логистическая регрессия измеряет связь между категориальной зависимой переменной и одной или несколькими независимыми переменными путем оценки вероятностей, используя логистическую/сигмоидную функцию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "__label__1    5097\n__label__2    4903\nName: label, dtype: int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.8516\n",
      "LR, WordLevel TF-IDF:  0.8528\n",
      "LR, N-Gram Vectors:  0.776\n",
      "LR, CharLevel Vectors:  0.8196\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier на Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier на Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier на Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier на Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM\n",
    "\n",
    "SVM -  алгоритм машинного обучения с учителем, который может быть использован как для классификации, так и для регрессии. Модель извлекает наилучшую возможную гиперплоскость/линию, которая разделяет эти два класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.7716\n"
     ]
    }
   ],
   "source": [
    "# SVM на Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest\n",
    "\n",
    "Cлучайный лес - ансамбль решающих деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.8396\n",
      "RF, WordLevel TF-IDF:  0.8276\n"
     ]
    }
   ],
   "source": [
    "# RF на Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF на Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boosting Model\n",
    "\n",
    "Бустинг - процедура последовательного построения композиции алгоритмов машинного обучения, когда каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\PROFES~1\\AppData\\Local\\Temp/ipykernel_14936/1244480762.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mxgboost\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extereme Gradient Boosting на Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting на Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting на Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ тональности\n",
    "\n",
    "Другая задача NLP - анализ тональности текста. Анализ тональности - определение полярности эмоциональных оценок в исследуемом тексте, который содержит мнения, суждения, эмоции, отношение автора к сущностям, личностям, вопросам, событиям, темам и их атрибутам.\n",
    "\n",
    "С помощью SentimentIntensityAnalyzer определим тональность отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sent_analysis = trainDF['text'].apply(sid.polarity_scores)\n",
    "sent_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainDF['text'][0])\n",
    "print(sent_analysis[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainDF['text'][10])\n",
    "print(sent_analysis[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization\n",
    "\n",
    "В заключении отобразим \n",
    "- самые популярные слова всех отзывов\n",
    "- самые популярные слова положительных отзывов\n",
    "- самые популярные слова отрицательных отзывов\n",
    "в видео облака слов с помощью WordCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "words = []\n",
    "for line in trainDF['text']: \n",
    "    line_words = line.split()\n",
    "    words.extend(line_words)\n",
    "\n",
    "wordfreq = collections.Counter(words)\n",
    "wordcloud = WordCloud(\n",
    " background_color='white',\n",
    " max_words=2000,\n",
    " stopwords=stopwords\n",
    " ).generate_from_frequencies(wordfreq)\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = []\n",
    "for line in trainDF['text'][trainDF['label']=='__label__2']: \n",
    "    line_words = line.split()\n",
    "    pos_words.extend(line_words)\n",
    "pos_words = ' '.join(pos_words)\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "# draw a Word Cloud with word frequencies\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=50000,\n",
    "    stopwords = stopwords_set\n",
    "   ).generate(pos_words)\n",
    "plt.figure(figsize=(13,12))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = [r for r in trainDF['text'][trainDF['label']=='__label__1']]\n",
    "neg = ''.join(negative)\n",
    "# draw a Word Cloud with word frequencies\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "wordcloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50000,\n",
    "    stopwords = stopwords_set\n",
    "   ).generate(neg)\n",
    "plt.figure(figsize=(13,12))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем на русском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(\"datasets/reviews_for_the_summer.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from string import punctuation\n",
    "\n",
    "exclude = set(punctuation)\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in exclude]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "data['text'] = data['Content'].apply(preprocess_text)\n",
    "data = data[data['Rating'] != 3]\n",
    "data['target'] = (data['Rating'] > 3)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = data['target'].astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size=0.2,\n",
    "                                                    random_state=13, stratify=data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = HashingVectorizer(n_features=200).fit(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = count_vect.transform(X_train)\n",
    "xtest = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LogisticRegression(class_weight=\"balanced\").fit(xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, lr.predict_proba(xtest)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-65960e07",
   "language": "python",
   "display_name": "PyCharm (introduction-to-natural-language-processing)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}