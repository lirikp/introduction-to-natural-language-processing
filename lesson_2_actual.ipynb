{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Создание признакового пространства "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы машинного обучения не могут напрямую работать с сырым текстом, поэтому необходимо конвертировать текст в наборы цифр (векторы). Это называется извлечением признаков.\n",
    "\n",
    "##### Мешок слов\n",
    "– это популярная и простая техника извлечения признаков, используемая при работе с текстом. Она описывает вхождения каждого слова в текст.\n",
    "\n",
    "Чтобы использовать модель, нам нужно:\n",
    "\n",
    "- Определить словарь известных слов (токенов).\n",
    "- Выбрать степень присутствия известных слов.\n",
    "\n",
    "Любая информация о порядке или структуре слов игнорируется. Вот почему это называется МЕШКОМ слов. Эта модель пытается понять, встречается ли знакомое слово в документе, но не знает, где именно оно встречается.\n",
    "\n",
    "Интуиция подсказывает, что схожие документы имеют схожее содержимое. Также, благодаря содержимому, мы можем узнать кое-что о смысле документа.\n",
    "\n",
    "Пример:\n",
    "Рассмотрим шаги создания этой модели. Мы используем только 4 предложения, чтобы понять, как работает модель. В реальной жизни вы столкнетесь с бОльшими объемами данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"I like this movie, it's it's it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем словарь и создаем векторы документа. Соберем все уникальные слова из 4 загруженных предложений, игнорируя регистр, пунктуацию и односимвольные токены. Это и будет наш словарь (известные слова).\n",
    "\n",
    "Для создания словаря можно использовать класс CountVectorizer из библиотеки sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[\"I like this movie, it's it's it's funny.\",\n 'I hate this movie.',\n 'This was awesome! I like it.',\n 'Nice one. I love it.']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `CountVectorizer` not found.\n"
     ]
    }
   ],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       a   aw   f   fu   h   ha   i   i    it  ...  ve   vi  vie  w  wa  was  \\\n0  7   0    0   1    1   0    0   3    0    3  ...    0   1    1  0   0    0   \n1  3   0    0   0    0   1    1   0    0    0  ...    0   1    1  0   0    0   \n2  5   1    1   0    0   0    0   2    1    1  ...    0   0    0  2   1    1   \n3  4   0    0   0    0   0    0   2    1    1  ...    1   0    0  0   0    0   \n\n   we  wes  y  y.  \n0   0    0  1   1  \n1   0    0  0   0  \n2   1    1  0   0  \n3   0    0  0   0  \n\n[4 rows x 142 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>a</th>\n      <th>aw</th>\n      <th>f</th>\n      <th>fu</th>\n      <th>h</th>\n      <th>ha</th>\n      <th>i</th>\n      <th>i</th>\n      <th>it</th>\n      <th>...</th>\n      <th>ve</th>\n      <th>vi</th>\n      <th>vie</th>\n      <th>w</th>\n      <th>wa</th>\n      <th>was</th>\n      <th>we</th>\n      <th>wes</th>\n      <th>y</th>\n      <th>y.</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 142 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=[1,3], analyzer='char', binary=False, tokenizer=str.split)\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда размер словаря увеличивается, вектор документа тоже растет. В примере выше, длина вектора равна количеству известных слов.\n",
    "\n",
    "В некоторых случаях, у нас может быть неимоверно большой объем данных и тогда вектор может состоять из тысяч или миллионов элементов. Более того, каждый документ может содержать лишь малую часть слов из словаря.\n",
    "\n",
    "Как следствие, в векторном представлении будет много нулей. Векторы с большим количеством нулей называются разреженным векторами (sparse vectors), они требуют больше памяти и вычислительных ресурсов. Частично эту проблему можно реить хорошей предобработкой.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Мешок N-грамм\n",
    "\n",
    "Другой, более сложный способ создания словаря – использовать сгруппированные слова. Это изменит размер словаря и даст мешку слов больше деталей о документе. Такой подход называется «N-грамма».\n",
    "\n",
    "N-грамма это последовательность каких-либо сущностей (слов, букв, чисел, цифр и т.д.). В контексте языковых корпусов, под N-граммой обычно понимают последовательность слов. Юниграмма это одно слово, биграмма это последовательность двух слов, триграмма – три слова и так далее. Цифра N обозначает, сколько сгруппированных слов входит в N-грамму. В модель попадают не все возможные N-граммы, а только те, что фигурируют в корпусе.\n",
    "\n",
    "Пример:\n",
    "Рассмотрим такое предложение:The office building is open today\n",
    "\n",
    "Вот его биграммы:\n",
    "\n",
    "- the office\n",
    "- office building\n",
    "- building is\n",
    "- is open\n",
    "- open today\n",
    "\n",
    "Как видно, мешок биграмм – это более действенный подход, чем мешок слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('I', 'like'),\n ('like', 'this'),\n ('this', 'movie,'),\n ('movie,', \"it's\"),\n (\"it's\", 'funny.'),\n ('funny.', 'I'),\n ('I', 'hate'),\n ('hate', 'this'),\n ('this', 'movie.'),\n ('movie.', 'This'),\n ('This', 'was'),\n ('was', 'awesome!'),\n ('awesome!', 'I'),\n ('I', 'like'),\n ('like', 'it.'),\n ('it.', 'Nice'),\n ('Nice', 'one.'),\n ('one.', 'I'),\n ('I', 'love'),\n ('love', 'it.')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
    "tokenized = text.split()\n",
    "bigrams = ngrams(tokenized, (2))\n",
    "list(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "У частотного скоринга есть проблема: слова с наибольшей частотностью имеют, соответственно, наибольшую оценку. В этих словах может быть не так много информационного выигрыша для модели, как в менее частых словах. Один из способов исправить ситуацию – понижать оценку слова, которое часто встречается во всех схожих документах. Это называется TF-IDF.\n",
    "\n",
    "TF-IDF (сокращение от term frequency — inverse document frequency) – это статистическая мера для оценки важности слова в документе, который является частью коллекции или корпуса.\n",
    "\n",
    "Скоринг по TF-IDF растет пропорционально частоте появления слова в документе, но это компенсируется количеством документов, содержащих это слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image/tf_idf.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `TfidfVectorizer` not found.\n"
     ]
    }
   ],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    awesome     funny      hate        it      like      love     movie  \\\n0  0.000000  0.812578  0.000000  0.259329  0.320323  0.000000  0.320323   \n1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n\n       nice       one      this       was  \n0  0.000000  0.000000  0.259329  0.000000  \n1  0.000000  0.000000  0.448100  0.000000  \n2  0.000000  0.000000  0.344321  0.539445  \n3  0.541736  0.541736  0.000000  0.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>awesome</th>\n      <th>funny</th>\n      <th>hate</th>\n      <th>it</th>\n      <th>like</th>\n      <th>love</th>\n      <th>movie</th>\n      <th>nice</th>\n      <th>one</th>\n      <th>this</th>\n      <th>was</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.812578</td>\n      <td>0.000000</td>\n      <td>0.259329</td>\n      <td>0.320323</td>\n      <td>0.000000</td>\n      <td>0.320323</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.259329</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.702035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.553492</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.448100</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.539445</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.344321</td>\n      <td>0.425305</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.344321</td>\n      <td>0.539445</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.345783</td>\n      <td>0.000000</td>\n      <td>0.541736</td>\n      <td>0.000000</td>\n      <td>0.541736</td>\n      <td>0.541736</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "document = [\"I like this movie, it's funny funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.91629073, 1.91629073, 1.91629073, 1.22314355, 1.51082562,\n       1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.22314355,\n       1.91629073])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчеты и частоты могут быть очень полезны, но одним из ограничений этих методов является то, что словарный запас может стать очень большим. Это, в свою очередь, потребует больших векторов для кодирования документов и налагает большие требования к памяти и замедляет алгоритмы.\n",
    "\n",
    "Умный обходной путь - использовать односторонний хэш слов, чтобы преобразовать их в целые числа. Умная часть заключается в том, что словарь не требуется, и вы можете выбрать произвольный длинный вектор фиксированной длины. Недостатком является то, что хеш является односторонней функцией, поэтому нет способа преобразовать кодировку обратно в слово (что может не иметь значения для многих контролируемых задач обучения).HashingVectorizer класс реализует этот подход, который можно использовать для последовательного хеширования слов, а затем для токенизации и кодирования документов по мере необходимости.\n",
    "\n",
    "Пример ниже демонстрирует HashingVectorizer для кодирования одного документа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `HashingVectorizer` not found.\n"
     ]
    }
   ],
   "source": [
    "HashingVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n",
      "[[ 0.37796447  0.          0.          0.          0.          0.\n",
      "   0.         -0.37796447  0.          0.         -0.37796447  0.\n",
      "   0.          0.          0.75592895  0.        ]\n",
      " [ 0.          0.          0.         -0.57735027  0.          0.\n",
      "   0.          0.          0.          0.         -0.57735027  0.\n",
      "   0.          0.          0.57735027  0.        ]\n",
      " [ 0.          0.4472136   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.89442719  0.        ]\n",
      " [ 0.          0.          0.          0.         -0.5         0.\n",
      "   0.          0.          0.         -0.5         0.          0.\n",
      "   0.          0.5         0.5         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "vectorizer = HashingVectorizer(n_features=2**4,)\n",
    "values = vectorizer.fit_transform(document)\n",
    "print(values.shape)\n",
    "print(values.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнение примера кодирует образец документа как разреженный массив из 16 элементов. Значения закодированного документа соответствуют нормализованному количеству слов по умолчанию в диапазоне от -1 до 1, но могут быть сделаны простые целочисленные счетчики путем изменения конфигурации по умолчанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Для чего нужны Vectorizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы машинного обучения не могут напрямую работать с сырым текстом, поэтому необходимо конвертировать текст в наборы цифр (векторы). Уже с векторным представлением можно производить классификацию, к примеру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text       label\n0  Stuning even for the non-gamer: This sound tra...  __label__2\n1  The best soundtrack ever to anything.: I'm rea...  __label__2\n2  Amazing!: This soundtrack is my favorite music...  __label__2\n3  Excellent Soundtrack: I truly like this soundt...  __label__2\n4  Remember, Pull Your Jaw Off The Floor After He...  __label__2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Stuning even for the non-gamer: This sound tra...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The best soundtrack ever to anything.: I'm rea...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Amazing!: This soundtrack is my favorite music...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Excellent Soundtrack: I truly like this soundt...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n      <td>__label__2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем данные\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, ..., 0, 0, 1])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(7500, 31681)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.876"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "__label__1    5097\n__label__2    4903\nName: label, dtype: int64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, ..., 0, 0, 1])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vct = HashingVectorizer(analyzer='word', token_pattern=r'\\w{1,}', n_features=200)\n",
    "vct.fit(trainDF['text'])\n",
    "\n",
    "xtrain_hash =  vct.transform(train_x)\n",
    "xvalid_hash =  vct.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_hash, train_y)\n",
    "predictions = classifier.predict(xvalid_hash)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7212"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(7500, 200)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_hash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw in train_data.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(16,), dtype=string, numpy=\n array([b'Garbage is exactly that......: I dont know what many people saw in this album, but i feel a rethink is due for. Appaling melodies, pathetic excuses for a beat. And the whining of some over privlaged rock stars who obviously feel as though they deserve more. Any attempt to throw in the odd tune is discarded for some strange attempt at \"art rock\" BORING',\n        b'Worthy of Several Academy Nominations: Director Susanne Bier, Best Screenplay writers Susanne Bier and Anders Thomas Jensen, Best Actor Rolf Lassgrd, Best Actress Sidse Babett Knudsen, and a few other supporting and camera work nominations would not surprise me. I will be disappointed if they are not rewarded for the talent and intelligence evident in this film. After the Wedding is so honestly human that it does indeed break your heart. Happily it also hands you hope, a little laughter, and moments of tender warmth and love.',\n        b'Relaxing entertainment: I did not realise that this was just a book containing the 3 other professor books that I already own and have read.I would not have bought this had I been aware.',\n        b\"Don't waste your time: Ridiculously hard to follow the random thoughts of Mr. Selengut. The constant questions and change of font made it even more strange. Reminded me of teachers I had that may have KNOWN the subject but couldn't TEACH it at all.\",\n        b'A little of everything: The plot and cinematography of this film were above average, while other aspects were a little lackluster. The storyline was well written, and the casting was on target, however the mixture of computer graphics took plenty away from the film and cheapened it quite a bit. Quite dissapointing.You do get to see Martin Short in a most unusual role, and I\\'d consider this a must-see for any admirer of his antics. Robert Pastorelli (the crazed painter from Murphy Brown) plays a rather lackluster role. Kathleen Turner pulled off the typical \"Evil Witch\" role a little too well. Mara Wilson seemed to do a better job in \"Matilda,\" but she was good in this film nontheless.I happened to get the Italian version of this film (alternately titled \"Un Semplice Desiderio\") and I will add that watching this movie with Italian audio on adds a little to it. Granted I may have never even seen this film if it werent in a lot of videos I bought from ebay.it :)',\n        b\"Protect a bed mattress cover: I purchase a Protect-A-Bed mattress cover at a mattress store, salesman said to wash it twice prior to use. He did not say to dry it with towels and sheets. Its supposed to be waterproof, one of our cats proved its not, contacted company, they asked if dried on medium with towels and other bedding, I said no, no one told me and not anywhere on package does it state needs to be protected in the dryer. Company sent another cover,this time on low setting which is mostly air fluff with towels and other sheets and it was scorched in 3 places. Both were scorched on medium and low heat, dryer is electric which is not as hot as gas. I'm getting my $90 back that I was charged at a mattress company. Not worth the money if you can't dry it because it has cheap plastic backing.\",\n        b'Very Educational!: I have two year old twin girls who are absolutely mesmorized by this film, every day they insist on watching it, and have learned how to handle situations involving a fire without even realizing it. They both claim they want to be fire fighters so they can \"put out the fire in Hooper\\'s Store\". I highly reccomend this movie to any parent wanting to teach their child about fire safety.',\n        b'Not compatible with Jawbone Icon: With the rubber ring in place the head set does not seat well in the gels and comes out too easily. With the ring off it will barely stay in the small size. The medium gels fit me best until they are stretched by the rubber ring. One gel even split wen trying to put the earpiece back in. I am trying the small for now until I find a better solution.',\n        b\"might be a good scanner: my scanner hasn't worked since the day i took it out of the box, installed the software, and the green light is on but when i hit scan it does nothing! looks like it's a dud. lol.\",\n        b\"This is the type of 'Garbage' you don't throw away!: Hello = )if your looking for great music, then you need this CD , you won't be dissapointed by it. 'Milk'is my personal favorite, it makes me want to cry..tears of sympathy (and of joy) for this great song, i don't care who knows it..it's wonderful, and Shirley Manson has a great voice for all who appreciate great voices, she can really sing, and touch your soul. Have a nice day.\",\n        b\"A rare & wondeful treat: I was extremely lucky to have found this, because it is a) an import & b) limited edition...especially lucky to find just thrown in the kylie section of a Pennsylvania Borders! Ok, here it is track-by-track:1. Chocolate [Radio Edit]:While this cuts the album version of the song by one minute and two seconds, it's better. Her vocals are improved, & more of an arrangement is used. 10/102. City Games: THis extreme rarity is fabulous. Very R&B; pop. 10/103. Chocolate [Tom Middleton Cosmos Remix]: A club dub. Cool when you have so much energy in you that you have to dance it out, but not for a regular listen. 7/104. Chocolate [EMO Mix]: Woderful. A good R&B; dance mix of the song. 9/105. Love @ First Sight [Live]: A live version of the Fever hit. I usually love live sounds, but this when isn't quite up to par with the original. 8/10I still have yet to figure out how to access the video. (...)Overall great package, great mixes, great rarities.\",\n        b\"Disappointing: At first, I was happy with the product because my dog wasn't pulling nearly as hard as before. But then I noticed that the side strap pushes up into her eye, making it water and become irritated, when we walk. I tried fixing the problem but nothing worked, it kept going into her eye. The product was definitely a waste of money.\",\n        b\"Cheaper (and maybe better than) a Therapist: Though not a fan of his astrological writings for Vanity Fair, this book is mind-blowing. I have had it for over 20 years (even had it rebound at Kinkos). It has truly helped me reinterpret the motivation of countless people in my life in a way that's helped me improve those relationships. It's a book like no other in the way it helps me make sense of other people's actions by understanding their point of view (haven't read his new book yet).\",\n        b'Producing foods which seem time-consuming but only take a fraction of the time: Semi-Homemade Cooking introduces the idea of producing foods which seem time-consuming but only take a fraction of the time. How? By blending 70% store-bought ingredients with 30% fresh foods to produce the taste of homemade. Open with a Raspberry Sake, move to a Crabby Bisque of condensed crab bisque enhanced with heavy cream and crabmeat, then use the slow cooker to produce Chinese Braised Short Ribs or Southern Pulled Pork - or top it off with Cinnamon Buns made from frozen white bread dough enhanced with spices, sugar, and butter.',\n        b'WWI movie review: This was an excellent movie that followed historical facts, more or less, and had superb flight sequences (that I assumed were computerized.) There was almost NO sexual content, and no adult language problems, so, excepting for some occasional war violence, the movie would be suitable for almost all ages. The backgrounds of the various characters were the usual \"composite characters\" that one would expect in a movie where disparate personalities came together to be molded into a \"team,\" but this was better than most. I probably should have given it 5 stars, but I reserve those ratings for the truly outstanding movie that comes along rarely.',\n        b\"SUPERB: This toy took a little getting use to. But after a few times playing with it I was amazing. I wish there were more options for the speed & vibration but It makes me quite happy & I'm not always easy to please. ;-)\"],\n       dtype=object)>,\n <tf.Tensor: shape=(16,), dtype=int32, numpy=array([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1])>)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "vocab_size = 10000\n",
    "seq_len = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=200\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(200, (3)),\n",
    "    Conv1D(200, (2)),\n",
    "    GRU(300),\n",
    "    #GlobalAveragePooling1D(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:394 call\n        outputs = layer(inputs, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py:426 call\n        inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:868 _process_inputs\n        initial_state = self.get_initial_state(inputs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:650 get_initial_state\n        init_state = get_initial_state_fn(\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:1963 get_initial_state\n        return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:2998 _generate_zero_filled_state_for_cell\n        return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:3016 _generate_zero_filled_state\n        return create_zeros(state_size)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:3011 create_zeros\n        return array_ops.zeros(init_state_size, dtype=dtype)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2911 wrapped\n        tensor = fun(*args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2960 zeros\n        output = _constant_if_small(zero, shape, dtype, name)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2896 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:5 prod\n        \n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3051 prod\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:867 __array__\n        raise NotImplementedError(\n\n    NotImplementedError: Cannot convert a symbolic Tensor (sequential/gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-30-99d3184347a1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalid_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1181\u001B[0m                 _r=1):\n\u001B[0;32m   1182\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1183\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1184\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1185\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    887\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    891\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    931\u001B[0m       \u001B[1;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    932\u001B[0m       \u001B[0minitializers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 933\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minitializers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    934\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    935\u001B[0m       \u001B[1;31m# At this point we know that the initialization is complete (or less\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[1;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[0;32m    761\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph_deleter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFunctionDeleter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lifted_initializer_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    762\u001B[0m     self._concrete_stateful_fn = (\n\u001B[1;32m--> 763\u001B[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    764\u001B[0m             *args, **kwds))\n\u001B[0;32m    765\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3048\u001B[0m       \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3049\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3050\u001B[1;33m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3051\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3052\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   3442\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3443\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3444\u001B[1;33m           \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3445\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3446\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   3277\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3278\u001B[0m     graph_function = ConcreteFunction(\n\u001B[1;32m-> 3279\u001B[1;33m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[0;32m   3280\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3281\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m    997\u001B[0m         \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    998\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 999\u001B[1;33m       \u001B[0mfunc_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1000\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1001\u001B[0m       \u001B[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m    670\u001B[0m         \u001B[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    671\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcompile_with_xla\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 672\u001B[1;33m           \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    673\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    674\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mO:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    984\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    985\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ag_error_metadata\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 986\u001B[1;33m               \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    987\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    988\u001B[0m               \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: in user code:\n\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:394 call\n        outputs = layer(inputs, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py:426 call\n        inputs, initial_state, _ = self._process_inputs(inputs, initial_state, None)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:868 _process_inputs\n        initial_state = self.get_initial_state(inputs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:650 get_initial_state\n        init_state = get_initial_state_fn(\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:1963 get_initial_state\n        return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:2998 _generate_zero_filled_state_for_cell\n        return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:3016 _generate_zero_filled_state\n        return create_zeros(state_size)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:3011 create_zeros\n        return array_ops.zeros(init_state_size, dtype=dtype)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2911 wrapped\n        tensor = fun(*args, **kwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2960 zeros\n        output = _constant_if_small(zero, shape, dtype, name)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2896 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:5 prod\n        \n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3051 prod\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    O:\\Users\\Professional\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:867 __array__\n        raise NotImplementedError(\n\n    NotImplementedError: Cannot convert a symbolic Tensor (sequential/gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=valid_data, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim, name=\"embedding\")\n",
    "        self.conv1 = Conv1D(200, (3))\n",
    "        self.conv2 = Conv1D(200, (3))\n",
    "        self.gPool = GlobalAveragePooling1D()\n",
    "        self.fc1 = Dense(100, activation='relu')\n",
    "        self.fc2 = Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        x1 = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gPool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ss(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = myNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "469/469 [==============================] - 13s 25ms/step - loss: 0.5852 - accuracy: 0.6378 - val_loss: 0.4185 - val_accuracy: 0.8160\n",
      "Epoch 2/15\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.2444 - accuracy: 0.9018 - val_loss: 0.4274 - val_accuracy: 0.8320\n",
      "Epoch 3/15\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.1097 - accuracy: 0.9640 - val_loss: 0.5540 - val_accuracy: 0.7940\n",
      "Epoch 4/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.7106 - val_accuracy: 0.8244\n",
      "Epoch 5/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0567 - accuracy: 0.9782 - val_loss: 0.7463 - val_accuracy: 0.8212\n",
      "Epoch 6/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0328 - accuracy: 0.9883 - val_loss: 0.9834 - val_accuracy: 0.8168\n",
      "Epoch 7/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0224 - accuracy: 0.9909 - val_loss: 1.3219 - val_accuracy: 0.8176\n",
      "Epoch 8/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0226 - accuracy: 0.9910 - val_loss: 1.6931 - val_accuracy: 0.8136\n",
      "Epoch 9/15\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 1.3695 - val_accuracy: 0.8124\n",
      "Epoch 10/15\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0185 - accuracy: 0.9919 - val_loss: 1.5048 - val_accuracy: 0.8088\n",
      "Epoch 11/15\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0131 - accuracy: 0.9947 - val_loss: 1.6104 - val_accuracy: 0.8144\n",
      "Epoch 12/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 1.8515 - val_accuracy: 0.8116\n",
      "Epoch 13/15\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0152 - accuracy: 0.9943 - val_loss: 1.0096 - val_accuracy: 0.7672\n",
      "Epoch 14/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0152 - accuracy: 0.9925 - val_loss: 1.7764 - val_accuracy: 0.8172\n",
      "Epoch 15/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 1.5168 - val_accuracy: 0.8088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5cbc4745b0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit(train_data, validation_data=valid_data, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}