{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1. Создание признакового пространства\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "### 2. Подгрузка библиотек и данных\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_data = pd.read_pickle(\"./train_data.pkl\")\n",
    "test_data = pd.read_pickle(\"./test_data.pkl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "Исключим стоп-слова с помощью stop_words='english'.\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       aareareived  aaree  aareoyound  able  aboyout  absolyoutely  accoyount  \\\n0                0      0           0     0        0             0          0   \n1                0      0           0     0        0             0          0   \n2                0      0           0     0        0             0          0   \n3                0      0           0     0        0             0          0   \n4                0      0           0     0        0             0          0   \n...            ...    ...         ...   ...      ...           ...        ...   \n31957            0      0           0     0        0             0          0   \n31958            0      0           0     0        0             0          0   \n31959            0      0           0     0        0             0          0   \n31960            0      0           0     0        0             0          0   \n31961            0      0           0     0        0             0          0   \n\n       act  actoare  actyoually  ...  yoyoultyouaree  \\\n0        0        0           0  ...               0   \n1        0        0           0  ...               0   \n2        0        0           0  ...               0   \n3        0        0           0  ...               0   \n4        0        0           0  ...               0   \n...    ...      ...         ...  ...             ...   \n31957    0        0           0  ...               0   \n31958    0        0           0  ...               0   \n31959    0        0           0  ...               0   \n31960    1        0           0  ...               0   \n31961    0        0           0  ...               0   \n\n       yoyoultyouareeofdevelopment  yoyoungarein  yoyous  yoyoustomeare  \\\n0                                0             0       0              0   \n1                                0             0       0              0   \n2                                0             0       0              0   \n3                                0             0       0              0   \n4                                0             0       0              0   \n...                            ...           ...     ...            ...   \n31957                            0             0       0              0   \n31958                            0             0       0              0   \n31959                            0             0       0              0   \n31960                            0             0       0              0   \n31961                            0             0       0              0   \n\n       yoyoustomeares  yoyout  yoyoute  yoyoutyoube  yyoummy  \n0                   0       0        0            0        0  \n1                   0       0        0            0        0  \n2                   0       0        0            0        0  \n3                   0       0        0            0        0  \n4                   0       0        0            0        0  \n...               ...     ...      ...          ...      ...  \n31957               0       0        0            0        0  \n31958               0       0        0            0        0  \n31959               0       0        0            0        0  \n31960               0       0        0            0        0  \n31961               0       0        0            0        0  \n\n[31962 rows x 1000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aareareived</th>\n      <th>aaree</th>\n      <th>aareoyound</th>\n      <th>able</th>\n      <th>aboyout</th>\n      <th>absolyoutely</th>\n      <th>accoyount</th>\n      <th>act</th>\n      <th>actoare</th>\n      <th>actyoually</th>\n      <th>...</th>\n      <th>yoyoultyouaree</th>\n      <th>yoyoultyouareeofdevelopment</th>\n      <th>yoyoungarein</th>\n      <th>yoyous</th>\n      <th>yoyoustomeare</th>\n      <th>yoyoustomeares</th>\n      <th>yoyout</th>\n      <th>yoyoute</th>\n      <th>yoyoutyoube</th>\n      <th>yyoummy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31957</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31958</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31959</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31960</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31961</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>31962 rows × 1000 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def vectorizer(data, obj_vectorizer):\n",
    "    document = []\n",
    "    for stemmer in data:\n",
    "        document.append(' '.join(stemmer))\n",
    "    # Создаем the Bag-of-Words модель\n",
    "    bag_of_words = obj_vectorizer.fit_transform(document)\n",
    "\n",
    "    # Отобразим Bag-of-Words модель как DataFrame\n",
    "    feature_names = obj_vectorizer.get_feature_names()\n",
    "    \n",
    "    return bag_of_words, pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=[1,1], \n",
    "                                   max_df=0.9, \n",
    "                                   max_features = 1000, \n",
    "                                   stop_words='english', \n",
    "                                   analyzer='word', \n",
    "                                   binary=False, \n",
    "                                   tokenizer=str.split)\n",
    "\n",
    "train_data_bag_of_words_countV_stemmed, out = vectorizer(train_data.tweet_stemmed, count_vectorizer)\n",
    "out\n",
    "train_data_bag_of_words_countV_lemmatized, out = vectorizer(train_data.tweet_lemmatized, count_vectorizer)\n",
    "out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "Исключим стоп-слова с помощью stop_words='english'.\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       aareareived  aaree  aaree yoyou  aareoyound  aareoyound city  able  \\\n0              0.0    0.0          0.0         0.0              0.0   0.0   \n1              0.0    0.0          0.0         0.0              0.0   0.0   \n2              0.0    0.0          0.0         0.0              0.0   0.0   \n3              0.0    0.0          0.0         0.0              0.0   0.0   \n4              0.0    0.0          0.0         0.0              0.0   0.0   \n...            ...    ...          ...         ...              ...   ...   \n31957          0.0    0.0          0.0         0.0              0.0   0.0   \n31958          0.0    0.0          0.0         0.0              0.0   0.0   \n31959          0.0    0.0          0.0         0.0              0.0   0.0   \n31960          0.0    0.0          0.0         0.0              0.0   0.0   \n31961          0.0    0.0          0.0         0.0              0.0   0.0   \n\n       aboyout  act  actyoually  adapt  ...  yoyouare life  yoyouareself  \\\n0          0.0  0.0         0.0    0.0  ...            0.0           0.0   \n1          0.0  0.0         0.0    0.0  ...            0.0           0.0   \n2          0.0  0.0         0.0    0.0  ...            0.0           0.0   \n3          0.0  0.0         0.0    0.0  ...            0.0           0.0   \n4          0.0  0.0         0.0    0.0  ...            0.0           0.0   \n...        ...  ...         ...    ...  ...            ...           ...   \n31957      0.0  0.0         0.0    0.0  ...            0.0           0.0   \n31958      0.0  0.0         0.0    0.0  ...            0.0           0.0   \n31959      0.0  0.0         0.0    0.0  ...            0.0           0.0   \n31960      0.0  1.0         0.0    0.0  ...            0.0           0.0   \n31961      0.0  0.0         0.0    0.0  ...            0.0           0.0   \n\n       yoyoultyouareeofdevelopment  \\\n0                              0.0   \n1                              0.0   \n2                              0.0   \n3                              0.0   \n4                              0.0   \n...                            ...   \n31957                          0.0   \n31958                          0.0   \n31959                          0.0   \n31960                          0.0   \n31961                          0.0   \n\n       yoyoultyouareeofdevelopment oaregareinanizations  yoyoungarein  \\\n0                                                   0.0           0.0   \n1                                                   0.0           0.0   \n2                                                   0.0           0.0   \n3                                                   0.0           0.0   \n4                                                   0.0           0.0   \n...                                                 ...           ...   \n31957                                               0.0           0.0   \n31958                                               0.0           0.0   \n31959                                               0.0           0.0   \n31960                                               0.0           0.0   \n31961                                               0.0           0.0   \n\n       yoyoustomeare  yoyout  yoyoute  yoyoute beayoutifyoul  yoyoutyoube  \n0                0.0     0.0      0.0                    0.0          0.0  \n1                0.0     0.0      0.0                    0.0          0.0  \n2                0.0     0.0      0.0                    0.0          0.0  \n3                0.0     0.0      0.0                    0.0          0.0  \n4                0.0     0.0      0.0                    0.0          0.0  \n...              ...     ...      ...                    ...          ...  \n31957            0.0     0.0      0.0                    0.0          0.0  \n31958            0.0     0.0      0.0                    0.0          0.0  \n31959            0.0     0.0      0.0                    0.0          0.0  \n31960            0.0     0.0      0.0                    0.0          0.0  \n31961            0.0     0.0      0.0                    0.0          0.0  \n\n[31962 rows x 1000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aareareived</th>\n      <th>aaree</th>\n      <th>aaree yoyou</th>\n      <th>aareoyound</th>\n      <th>aareoyound city</th>\n      <th>able</th>\n      <th>aboyout</th>\n      <th>act</th>\n      <th>actyoually</th>\n      <th>adapt</th>\n      <th>...</th>\n      <th>yoyouare life</th>\n      <th>yoyouareself</th>\n      <th>yoyoultyouareeofdevelopment</th>\n      <th>yoyoultyouareeofdevelopment oaregareinanizations</th>\n      <th>yoyoungarein</th>\n      <th>yoyoustomeare</th>\n      <th>yoyout</th>\n      <th>yoyoute</th>\n      <th>yoyoute beayoutifyoul</th>\n      <th>yoyoutyoube</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31957</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31958</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31959</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31960</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31961</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>31962 rows × 1000 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=[1,2], \n",
    "                                   max_df=0.9, \n",
    "                                   max_features = 1000, \n",
    "                                   stop_words='english', \n",
    "                                   analyzer='word', \n",
    "                                   binary=False, \n",
    "                                   tokenizer=str.split)\n",
    "\n",
    "train_data_bag_of_words_TF_IDF_stemmed, out = vectorizer(train_data.tweet_stemmed, tf_idf_vectorizer)\n",
    "out\n",
    "train_data_bag_of_words_TF_IDF_lemmatized, out = vectorizer(train_data.tweet_lemmatized, tf_idf_vectorizer)\n",
    "out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, \n",
    "составьте таблицу метод векторизации и скор который вы получили \n",
    "(в методах векторизации по изменяйте параметры что бы добиться лучшего скора) \n",
    "обратите внимание как падает/растёт скор при уменьшении количества фичей, \n",
    "и изменении параметров, так же попробуйте применить к векторайзерам PCA \n",
    "для сокращения размерности посмотрите на качество сделайте выводы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text       label\n0  Stuning even for the non-gamer: This sound tra...  __label__2\n1  The best soundtrack ever to anything.: I'm rea...  __label__2\n2  Amazing!: This soundtrack is my favorite music...  __label__2\n3  Excellent Soundtrack: I truly like this soundt...  __label__2\n4  Remember, Pull Your Jaw Off The Floor After He...  __label__2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Stuning even for the non-gamer: This sound tra...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The best soundtrack ever to anything.: I'm rea...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Amazing!: This soundtrack is my favorite music...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Excellent Soundtrack: I truly like this soundt...</td>\n      <td>__label__2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n      <td>__label__2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Загружаем данные\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "def getLRmodel(obj_vect,\n",
    "                train_x,\n",
    "                train_y,\n",
    "                valid_x,\n",
    "                valid_y):\n",
    "\n",
    "    obj_vect.fit(trainDF['text'])\n",
    "\n",
    "    xtrain_count =  obj_vect.transform(train_x)\n",
    "    xvalid_count =  obj_vect.transform(valid_x)\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(xtrain_count, train_y)\n",
    "    predictions = classifier.predict(xvalid_count)\n",
    "    #predictions\n",
    "    #xtrain_count.shape\n",
    "    return accuracy_score(valid_y, predictions)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_vect(default):  0.7984\n",
      "count_vect(Max features UP 2000):  0.8204\n",
      "count_vect(Max features UP 10000):  0.834\n",
      "count_vect(Max features DOWN 500):  0.7872\n",
      "count_vect(ngram_range [1,2]):  0.7976\n",
      "count_vect(ngram_range [1,3]):  0.7984\n",
      "count_vect(analyzer = 'char'):  0.5632\n",
      "count_vect(max_df=0.5):  0.7984\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=[1,1], max_df=0.9,max_features = 1000,stop_words='english',analyzer='word',binary=False,tokenizer=str.split)\n",
    "print(\"count_vect(default): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "count_vectorizer.max_features = 2000\n",
    "print(\"count_vect(Max features UP 2000): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "count_vectorizer.max_features = 10000\n",
    "print(\"count_vect(Max features UP 10000): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "count_vectorizer.max_features = 500\n",
    "print(\"count_vect(Max features DOWN 500): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "count_vectorizer.max_features = 1000\n",
    "count_vectorizer.ngram_range = [1, 2]\n",
    "print(\"count_vect(ngram_range [1,2]): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "count_vectorizer.ngram_range = [1, 3]\n",
    "print(\"count_vect(ngram_range [1,3]): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "count_vectorizer.ngram_range = [1, 1]\n",
    "count_vectorizer.analyzer = 'char'\n",
    "print(\"count_vect(analyzer = 'char'): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "count_vectorizer.analyzer = 'word'\n",
    "\n",
    "count_vectorizer.max_df = .5\n",
    "print(\"count_vect(max_df=0.5): \", getLRmodel(count_vectorizer, train_x, train_y, valid_x, valid_y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_idf_vect(default):  0.8052\n",
      "tf_idf_vect(Max features UP 2000):  0.8352\n",
      "tf_idf_vect(Max features UP 10000):  0.8532\n",
      "tf_idf_vect(Max features DOWN 500):  0.7908\n",
      "tf_idf_vect(ngram_range [1,1]):  0.8084\n",
      "tf_idf_vect(ngram_range [1,3]):  0.806\n",
      "tf_idf_vect(analyzer = 'char'):  0.5652\n",
      "tf_idf_vect(max_df=0.5):  0.8084\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=[1,2],\n",
    "                                   max_df=0.9,\n",
    "                                   max_features = 1000,\n",
    "                                   stop_words='english',\n",
    "                                   analyzer='word',\n",
    "                                   binary=False,\n",
    "                                   tokenizer=str.split)\n",
    "\n",
    "print(\"tf_idf_vect(default): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "tf_idf_vectorizer.max_features = 2000\n",
    "print(\"tf_idf_vect(Max features UP 2000): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "tf_idf_vectorizer.max_features = 10000\n",
    "print(\"tf_idf_vect(Max features UP 10000): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "tf_idf_vectorizer.max_features = 500\n",
    "print(\"tf_idf_vect(Max features DOWN 500): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "tf_idf_vectorizer.max_features = 1000\n",
    "tf_idf_vectorizer.ngram_range = [1, 1]\n",
    "print(\"tf_idf_vect(ngram_range [1,1]): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "tf_idf_vectorizer.ngram_range = [1, 3]\n",
    "print(\"tf_idf_vect(ngram_range [1,3]): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "\n",
    "tf_idf_vectorizer.ngram_range = [1, 1]\n",
    "tf_idf_vectorizer.analyzer = 'char'\n",
    "print(\"tf_idf_vect(analyzer = 'char'): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))\n",
    "tf_idf_vectorizer.analyzer = 'word'\n",
    "\n",
    "tf_idf_vectorizer.max_df = .5\n",
    "print(\"tf_idf_vect(max_df=0.5): \", getLRmodel(tf_idf_vectorizer, train_x, train_y, valid_x, valid_y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}